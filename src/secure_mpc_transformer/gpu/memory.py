"""
GPU memory management for secure MPC operations.
"""

import logging
from typing import Dict, List, Optional, Tuple, Union

import torch

logger = logging.getLogger(__name__)


class GPUMemoryManager:
    """
    Manages GPU memory allocation and optimization for secure computation.
    
    Provides memory pooling, caching, and optimization strategies
    specifically designed for MPC workloads.
    """
    
    def __init__(self, device: Optional[torch.device] = None, memory_fraction: float = 0.9):
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.memory_fraction = memory_fraction
        
        # Memory pools for different tensor types
        self.memory_pools: Dict[str, List[torch.Tensor]] = {
            'shares': [],
            'intermediates': [],
            'activations': [],
            'gradients': []
        }
        
        # Memory usage tracking
        self.allocated_memory = 0
        self.peak_memory = 0
        self.total_allocations = 0
        self.total_deallocations = 0
        
        # GPU memory properties
        if self.device.type == 'cuda':
            self.total_memory = torch.cuda.get_device_properties(self.device.index or 0).total_memory
            self.max_memory = int(self.total_memory * memory_fraction)
        else:
            self.total_memory = 0
            self.max_memory = 0
        
        logger.info(f\"Initialized GPU memory manager on {self.device}\")\n        if self.total_memory > 0:\n            logger.info(f\"Total GPU memory: {self.total_memory / 1e9:.2f} GB\")\n            logger.info(f\"Max usable memory: {self.max_memory / 1e9:.2f} GB\")\n    \n    def allocate_tensor(\n        self, \n        shape: Tuple[int, ...], \n        dtype: torch.dtype = torch.float32,\n        pool_type: str = 'intermediates',\n        initialize_zero: bool = True\n    ) -> torch.Tensor:\n        \"\"\"\n        Allocate a tensor with memory pool management.\n        \n        Args:\n            shape: Tensor shape\n            dtype: Data type\n            pool_type: Type of memory pool to use\n            initialize_zero: Whether to initialize with zeros\n            \n        Returns:\n            Allocated tensor\n        \"\"\"\n        # Try to reuse tensor from pool\n        reused_tensor = self._try_reuse_tensor(shape, dtype, pool_type)\n        if reused_tensor is not None:\n            if initialize_zero:\n                reused_tensor.zero_()\n            return reused_tensor\n        \n        # Check memory constraints\n        tensor_size = self._calculate_tensor_size(shape, dtype)\n        if not self._check_memory_available(tensor_size):\n            # Try garbage collection and pool cleanup\n            self._cleanup_memory_pools()\n            torch.cuda.empty_cache() if self.device.type == 'cuda' else None\n            \n            if not self._check_memory_available(tensor_size):\n                raise RuntimeError(f\"Insufficient GPU memory for tensor of size {tensor_size / 1e6:.2f} MB\")\n        \n        # Allocate new tensor\n        if initialize_zero:\n            tensor = torch.zeros(shape, dtype=dtype, device=self.device)\n        else:\n            tensor = torch.empty(shape, dtype=dtype, device=self.device)\n        \n        # Update memory tracking\n        self.allocated_memory += tensor_size\n        self.total_allocations += 1\n        self.peak_memory = max(self.peak_memory, self.allocated_memory)\n        \n        logger.debug(f\"Allocated tensor {shape} ({tensor_size / 1e6:.2f} MB) in pool {pool_type}\")\n        \n        return tensor\n    \n    def deallocate_tensor(self, tensor: torch.Tensor, pool_type: str = 'intermediates'):\n        \"\"\"\n        Deallocate tensor and optionally return to pool.\n        \n        Args:\n            tensor: Tensor to deallocate\n            pool_type: Pool type for potential reuse\n        \"\"\"\n        tensor_size = self._calculate_tensor_size(tensor.shape, tensor.dtype)\n        \n        # Add to pool if within size limits\n        if self._should_pool_tensor(tensor, pool_type):\n            self.memory_pools[pool_type].append(tensor.detach())\n            logger.debug(f\"Returned tensor {tensor.shape} to pool {pool_type}\")\n        else:\n            # Update memory tracking\n            self.allocated_memory -= tensor_size\n            self.total_deallocations += 1\n        \n        del tensor\n    \n    def allocate_share_tensors(\n        self, \n        shape: Tuple[int, ...], \n        num_shares: int,\n        dtype: torch.dtype = torch.float32\n    ) -> List[torch.Tensor]:\n        \"\"\"\n        Allocate multiple tensors for secret shares.\n        \n        Args:\n            shape: Shape of each share\n            num_shares: Number of shares to allocate\n            dtype: Data type\n            \n        Returns:\n            List of allocated share tensors\n        \"\"\"\n        shares = []\n        for i in range(num_shares):\n            share = self.allocate_tensor(shape, dtype, pool_type='shares')\n            shares.append(share)\n        \n        logger.debug(f\"Allocated {num_shares} share tensors of shape {shape}\")\n        return shares\n    \n    def deallocate_share_tensors(self, shares: List[torch.Tensor]):\n        \"\"\"\n        Deallocate multiple share tensors.\n        \n        Args:\n            shares: List of share tensors to deallocate\n        \"\"\"\n        for share in shares:\n            self.deallocate_tensor(share, pool_type='shares')\n        \n        logger.debug(f\"Deallocated {len(shares)} share tensors\")\n    \n    def create_activation_buffer(\n        self, \n        max_batch_size: int,\n        seq_length: int,\n        hidden_size: int,\n        num_layers: int,\n        dtype: torch.dtype = torch.float32\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Create pre-allocated buffers for transformer activations.\n        \n        Args:\n            max_batch_size: Maximum batch size\n            seq_length: Sequence length\n            hidden_size: Hidden dimension size\n            num_layers: Number of transformer layers\n            dtype: Data type\n            \n        Returns:\n            Dictionary of pre-allocated activation buffers\n        \"\"\"\n        buffers = {}\n        \n        # Main activation buffer\n        activation_shape = (max_batch_size, seq_length, hidden_size)\n        buffers['activations'] = self.allocate_tensor(\n            activation_shape, dtype, pool_type='activations'\n        )\n        \n        # Attention buffers\n        num_heads = 12  # Typical for BERT-base\n        head_dim = hidden_size // num_heads\n        \n        attention_shape = (max_batch_size, num_heads, seq_length, seq_length)\n        buffers['attention_scores'] = self.allocate_tensor(\n            attention_shape, dtype, pool_type='activations'\n        )\n        \n        query_key_value_shape = (max_batch_size, seq_length, hidden_size)\n        buffers['queries'] = self.allocate_tensor(\n            query_key_value_shape, dtype, pool_type='activations'\n        )\n        buffers['keys'] = self.allocate_tensor(\n            query_key_value_shape, dtype, pool_type='activations'\n        )\n        buffers['values'] = self.allocate_tensor(\n            query_key_value_shape, dtype, pool_type='activations'\n        )\n        \n        # Feed-forward buffers\n        ff_hidden_size = hidden_size * 4  # Typical expansion factor\n        ff_shape = (max_batch_size, seq_length, ff_hidden_size)\n        buffers['ff_intermediate'] = self.allocate_tensor(\n            ff_shape, dtype, pool_type='activations'\n        )\n        \n        total_memory = sum(\n            self._calculate_tensor_size(buf.shape, buf.dtype) \n            for buf in buffers.values()\n        )\n        \n        logger.info(f\"Created activation buffers: {total_memory / 1e6:.2f} MB total\")\n        \n        return buffers\n    \n    def optimize_memory_layout(self, tensors: List[torch.Tensor]) -> List[torch.Tensor]:\n        \"\"\"\n        Optimize memory layout of tensors for better GPU performance.\n        \n        Args:\n            tensors: List of tensors to optimize\n            \n        Returns:\n            List of optimized tensors\n        \"\"\"\n        optimized_tensors = []\n        \n        for tensor in tensors:\n            # Ensure tensors are contiguous\n            if not tensor.is_contiguous():\n                tensor = tensor.contiguous()\n            \n            # Optimize for GPU memory coalescing\n            optimized_tensor = self._optimize_tensor_layout(tensor)\n            optimized_tensors.append(optimized_tensor)\n        \n        return optimized_tensors\n    \n    def _optimize_tensor_layout(self, tensor: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Optimize individual tensor memory layout.\n        \n        Args:\n            tensor: Tensor to optimize\n            \n        Returns:\n            Optimized tensor\n        \"\"\"\n        # For GPU efficiency, ensure proper alignment\n        if self.device.type == 'cuda':\n            # Ensure tensor is aligned to GPU memory boundaries\n            # This is automatically handled by PyTorch in most cases\n            return tensor.contiguous()\n        \n        return tensor\n    \n    def _try_reuse_tensor(\n        self, \n        shape: Tuple[int, ...], \n        dtype: torch.dtype, \n        pool_type: str\n    ) -> Optional[torch.Tensor]:\n        \"\"\"\n        Try to reuse a tensor from the memory pool.\n        \n        Args:\n            shape: Required tensor shape\n            dtype: Required data type\n            pool_type: Pool to search\n            \n        Returns:\n            Reused tensor if available, None otherwise\n        \"\"\"\n        if pool_type not in self.memory_pools:\n            return None\n        \n        pool = self.memory_pools[pool_type]\n        \n        # Look for exact match\n        for i, tensor in enumerate(pool):\n            if tensor.shape == shape and tensor.dtype == dtype:\n                return pool.pop(i)\n        \n        # Look for larger tensor that can be sliced\n        for i, tensor in enumerate(pool):\n            if (tensor.dtype == dtype and \n                tensor.numel() >= torch.prod(torch.tensor(shape)) and\n                self._can_reshape_tensor(tensor, shape)):\n                \n                pool.pop(i)\n                return tensor.view(shape)\n        \n        return None\n    \n    def _can_reshape_tensor(self, tensor: torch.Tensor, target_shape: Tuple[int, ...]) -> bool:\n        \"\"\"\n        Check if tensor can be reshaped to target shape.\n        \n        Args:\n            tensor: Source tensor\n            target_shape: Target shape\n            \n        Returns:\n            True if reshaping is possible\n        \"\"\"\n        return tensor.numel() == torch.prod(torch.tensor(target_shape))\n    \n    def _should_pool_tensor(self, tensor: torch.Tensor, pool_type: str) -> bool:\n        \"\"\"\n        Determine if tensor should be added to pool for reuse.\n        \n        Args:\n            tensor: Tensor to check\n            pool_type: Pool type\n            \n        Returns:\n            True if tensor should be pooled\n        \"\"\"\n        # Don't pool very large tensors\n        tensor_size = self._calculate_tensor_size(tensor.shape, tensor.dtype)\n        max_pool_size = 50 * 1024 * 1024  # 50 MB\n        \n        if tensor_size > max_pool_size:\n            return False\n        \n        # Don't pool if pool is already full\n        max_pool_tensors = 100\n        if len(self.memory_pools.get(pool_type, [])) >= max_pool_tensors:\n            return False\n        \n        return True\n    \n    def _calculate_tensor_size(self, shape: Tuple[int, ...], dtype: torch.dtype) -> int:\n        \"\"\"\n        Calculate tensor size in bytes.\n        \n        Args:\n            shape: Tensor shape\n            dtype: Data type\n            \n        Returns:\n            Size in bytes\n        \"\"\"\n        num_elements = 1\n        for dim in shape:\n            num_elements *= dim\n        \n        dtype_size = torch.tensor([], dtype=dtype).element_size()\n        return num_elements * dtype_size\n    \n    def _check_memory_available(self, required_size: int) -> bool:\n        \"\"\"\n        Check if required memory is available.\n        \n        Args:\n            required_size: Required memory in bytes\n            \n        Returns:\n            True if memory is available\n        \"\"\"\n        if self.device.type == 'cuda':\n            current_memory = torch.cuda.memory_allocated(self.device)\n            available_memory = self.max_memory - current_memory\n            return available_memory >= required_size\n        \n        return True  # Assume CPU has sufficient memory\n    \n    def _cleanup_memory_pools(self):\n        \"\"\"Clean up memory pools to free space.\"\"\"\n        cleaned_memory = 0\n        \n        for pool_type, pool in self.memory_pools.items():\n            # Remove older tensors (simple FIFO)\n            while len(pool) > 10:  # Keep only recent tensors\n                tensor = pool.pop(0)\n                cleaned_memory += self._calculate_tensor_size(tensor.shape, tensor.dtype)\n                del tensor\n        \n        logger.debug(f\"Cleaned up {cleaned_memory / 1e6:.2f} MB from memory pools\")\n    \n    def get_memory_stats(self) -> Dict[str, Union[int, float]]:\n        \"\"\"\n        Get memory usage statistics.\n        \n        Returns:\n            Dictionary with memory statistics\n        \"\"\"\n        stats = {\n            'allocated_memory_mb': self.allocated_memory / 1e6,\n            'peak_memory_mb': self.peak_memory / 1e6,\n            'total_allocations': self.total_allocations,\n            'total_deallocations': self.total_deallocations,\n            'pool_sizes': {pool_type: len(pool) for pool_type, pool in self.memory_pools.items()}\n        }\n        \n        if self.device.type == 'cuda':\n            stats.update({\n                'gpu_allocated_mb': torch.cuda.memory_allocated(self.device) / 1e6,\n                'gpu_cached_mb': torch.cuda.memory_reserved(self.device) / 1e6,\n                'gpu_total_mb': self.total_memory / 1e6,\n                'gpu_utilization': torch.cuda.memory_allocated(self.device) / self.total_memory\n            })\n        \n        return stats\n    \n    def clear_all_pools(self):\n        \"\"\"Clear all memory pools.\"\"\"\n        for pool in self.memory_pools.values():\n            pool.clear()\n        \n        logger.info(\"Cleared all memory pools\")\n    \n    def force_garbage_collection(self):\n        \"\"\"Force garbage collection and memory cleanup.\"\"\"\n        import gc\n        \n        # Clear memory pools\n        self.clear_all_pools()\n        \n        # Python garbage collection\n        gc.collect()\n        \n        # GPU memory cleanup\n        if self.device.type == 'cuda':\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n        \n        logger.info(\"Forced garbage collection and memory cleanup\")"